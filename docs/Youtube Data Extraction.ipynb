{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Youtube Data Extraction: Easily Scrape All Videos And Comments From A Channel\n",
    "\n",
    "YouTube is one of the biggest platforms for content creation and consumption.\n",
    "\n",
    "And whether you're a data scientist, market researcher, or content creator, the ability to extract comprehensive data from a YouTube channel can provide invaluable insights. This tutorial introduces a powerful yet straightforward method to scrape all videos and comments from any YouTube channel.\n",
    "\n",
    "Using Python and the YouTube Data API v3, we'll build a robust scraper that can:\n",
    "\n",
    "1. Fetch all videos from a specified channel\n",
    "2. Extract detailed information for each video\n",
    "3. Collect all comments and replies for every video\n",
    "4. Store the data in an easily analyzable format\n",
    "\n",
    "Our approach prioritizes efficiency and simplicity, making it accessible for programmers of all skill levels. By the end of this guide, you'll have a versatile tool at your disposal, capable of gathering extensive YouTube data with minimal effort.\n",
    "\n",
    "Let's dive into the world of YouTube data extraction and unlock the potential of channel-wide content analysis!\n",
    "\n",
    "For this tutorial, all you'll need is a YouTube Data API v3 key to download the `api_crawler` package by using `pip install api_crawler`.\n",
    "\n",
    "\n",
    "\n",
    "## Setting Up\n",
    "\n",
    "First, let's import the necessary libraries and set up our API key:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_crawler import YoutubeAPI\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# OpenAI youtube channel ID\n",
    "channels_to_monitor = ['UCXZCJLdBC09xxGZ6gcdrc6A']\n",
    "\n",
    "YOUTUBE_V3_API_KEY = 'YOUR_YOUTUBE_DATA_V3_API_KEY'\n",
    "\n",
    "youtube_api = YoutubeAPI(api_key=YOUTUBE_V3_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Make sure to replace `'YOUR_YOUTUBE_DATA_V3_API_KEY'` with your actual API key.\n",
    "\n",
    "## Fetching Videos from the Channel\n",
    "\n",
    "Now that we've set up our environment, let's start by fetching all the videos from our target channel:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total videos: 131\n"
     ]
    }
   ],
   "source": [
    "videos = youtube_api.get_videos_from_channel(channels_to_monitor[0], get_full_info=True)\n",
    "\n",
    "print(f'Total videos: {len(videos)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code retrieves all videos from the specified channel, including detailed information for each video.\n",
    "\n",
    "## Collecting Comments for Each Video\n",
    "\n",
    "Next, we'll iterate through each video and collect all comments and replies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 0 videos\n",
      "Processed 25 videos\n",
      "Processed 50 videos\n",
      "Processed 75 videos\n",
      "Processed 100 videos\n",
      "Processed 125 videos\n"
     ]
    }
   ],
   "source": [
    "for i, video in enumerate(videos):\n",
    "    comments = youtube_api.get_all_comments(video['id'], include_metadata=True, include_replies = False)\n",
    "\n",
    "    comment_threads = [comment['id'] for comment in comments if comment.get('snippet', {}).get('totalReplyCount', 0) > 0]\n",
    "    \n",
    "    for comment_thread in comment_threads:\n",
    "        youtube_api.get_all_comment_replies(comment_thread)\n",
    "\n",
    "    if i % 25 == 0:\n",
    "        print(f'Processed {i} videos')\n",
    "        time.sleep(60*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loop collects all comments for each video and fetches replies for comments that have them. We've included a pause every 25 videos to avoid hitting API rate limits (which happens quite frequently if you're making too many requests).\n",
    "\n",
    "## Processing the Collected Data\n",
    "\n",
    "After collecting the data, we'll process it into pandas DataFrames for easy analysis.\n",
    "\n",
    "Notice that, in the code above, I am not saving the output to a list. And that's because the `api_crawler` package already saves the output to a json file to serves as a data lake.\n",
    "\n",
    "You could save the output to a list if you wanted, and that would not be wrong, but the entire context and metadata would be lost whenever you leave the notebook. That's why I prefer to use the local json data lake to save the output.\n",
    "\n",
    "In the code below, we will read the json data lake and process the data into a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from api_crawler.data_lake import read_log\n",
    "\n",
    "videos_log = [video \n",
    "              for output in read_log('lake/json_lakes/YoutubeAPI_get_videos_from_channel.json')\n",
    "              if output.get('output') is not None\n",
    "              for video in output['output']]\n",
    "\n",
    "\n",
    "videos_df = pd.json_normalize(videos_log)[['id', 'channel.id', 'channel.name','title', 'publishDate', 'duration.secondsText', 'viewCount.text']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process comment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_log = [comment \n",
    "                for output in read_log('lake/json_lakes/YoutubeAPI_get_all_comments.json')\n",
    "                if output.get('output') is not None\n",
    "                for comment in output['output']]\n",
    "\n",
    "comments_df = pd.json_normalize(comments_log)[['id', 'snippet.videoId','snippet.topLevelComment.snippet.authorDisplayName',\n",
    "                                               'snippet.topLevelComment.snippet.textDisplay', 'snippet.topLevelComment.snippet.publishedAt',\n",
    "                                               'snippet.topLevelComment.snippet.likeCount', 'snippet.totalReplyCount']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process comment replies data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_replies = [reply \n",
    "                    for comment in read_log('lake/json_lakes/YoutubeAPI_get_all_comment_replies.json')\n",
    "                    for reply in comment.get('output', []) if comment.get('output')]\n",
    "\n",
    "comment_thread_replies_df = pd.json_normalize(flattened_replies)[['id', 'snippet.parentId','snippet.authorDisplayName',\n",
    "                                                                  'snippet.textOriginal', 'snippet.publishedAt', 'snippet.likeCount']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Saving the Results\n",
    "\n",
    "Finally, we'll save our processed data to Excel files for further analysis:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_df.to_excel('openai_yt_videos.xlsx')\n",
    "comments_df.to_excel('openai_yt_comments.xlsx')\n",
    "comment_thread_replies_df.to_excel('openai_yt_comment_thread_replies.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there you have it!\n",
    "\n",
    "You've successfully scraped all videos and comments from a YouTube channel. The data is now ready for your analysis, whether you're looking at content trends, engagement metrics, or performing sentiment analysis on comments.\n",
    "\n",
    "For any channel you want to scrape, you can just replace/add the channel ID in the `channels_to_monitor` list and run the code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
